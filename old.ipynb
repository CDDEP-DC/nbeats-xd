{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# licensed under the Creative Commons - Attribution-NonCommercial 4.0\n",
    "# International license (CC BY-NC 4.0):\n",
    "# https://creativecommons.org/licenses/by-nc/4.0/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## enc-dec architecture, without error variance\n",
    "\n",
    "## models are saved in snapshots folder\n",
    "## this fn returns forecasts\n",
    "def train_model(model_name, iterations, vals_train, static_cat, lookback, loss_fn_name, use_static_cat, targ_name, exog_names, horizon, use_norm=True):\n",
    "\n",
    "    input_size = lookback * horizon  ## backward window size, in time units\n",
    "    ts_freq = 1 ## only needed for MASE loss\n",
    "    history_size_in_horizons = 60 ## model only uses this much history, max\n",
    "    use_vars = [targ_name, *exog_names]\n",
    "    \n",
    "    training_values = np.stack(\n",
    "                        [vals_train[k] for k in use_vars]\n",
    "                        ).transpose([1,2,0]) ## dims are [series, time, variables]\n",
    "                                            ## where the first variable is the target, and the rest are covariates\n",
    "    \n",
    "    n_features = training_values.shape[2] - 1 ## number of exogenous covariates\n",
    "    n_embed = static_cat.shape[0]\n",
    "\n",
    "    embed_dim = 2\n",
    "\n",
    "    ## TCN receptive field size = 2(k-1)(2^n - 1)+1 where k is kernel size and n is # blocks\n",
    "    tcn_k = 3\n",
    "    tcn_n = 4\n",
    "    tcn_dropout = 0.0\n",
    "    enc_hid = 128\n",
    "    enc_temporal = True\n",
    "    enc_dim = input_size if enc_temporal else enc_hid\n",
    "\n",
    "    if use_static_cat:\n",
    "        #exog_block = LSTM_back_cat(n_features=n_features,input_size=input_size,output_size=horizon,\n",
    "        #            layer_size=2048,n_embed=n_embed,embed_dim=embed_dim,decoder_extra_layers=0,lstm_layers=2,lstm_hidden=256)\n",
    "        #tcn_chans = n_features + embed_dim + 1\n",
    "        #exog_block = TCN(n_features, input_size, horizon, [enc_hid]*tcn_n, tcn_k, tcn_dropout, enc_temporal, n_embed, embed_dim)\n",
    "        exog_block = TCN_encoder(n_features, [enc_hid]*tcn_n, tcn_k, tcn_dropout, enc_temporal, n_embed, embed_dim)\n",
    "        #exog_block = LSTM_test(n_features=n_features,input_size=input_size,output_size=horizon,layer_size=-1,n_embed=n_embed,embed_dim=embed_dim,decoder_extra_layers=0,\n",
    "        #            lstm_layers=1,lstm_hidden=enc_hid,temporal=enc_temporal,decode=False)\n",
    "    else:\n",
    "        #exog_block = LSTM_with_backcast(n_features=n_features,input_size=input_size,output_size=horizon,\n",
    "        #            layer_size=2048,decoder_extra_layers=0,lstm_layers=2,lstm_hidden=256)\n",
    "        #tcn_chans = n_features + 1\n",
    "        #exog_block = TCN(n_features, input_size, horizon, [enc_hid]*tcn_n, tcn_k, tcn_dropout, enc_temporal)\n",
    "        exog_block = TCN_encoder(n_features, [enc_hid]*tcn_n, tcn_k, tcn_dropout, enc_temporal)\n",
    "        #exog_block = LSTM_test(n_features=n_features,input_size=input_size,output_size=horizon,layer_size=-1,decoder_extra_layers=0,\n",
    "        #            lstm_layers=1,lstm_hidden=enc_hid,temporal=enc_temporal,decode=False)\n",
    "    \n",
    "    #model = generic_exog(input_size=input_size, output_size=horizon,\n",
    "    #                stacks = 8,\n",
    "    #                layers = 4,  ## 4 per stack, from the paper\n",
    "    #                layer_size = -1, #128,\n",
    "    #                exog_block = exog_block,\n",
    "    #                exog_first=True,use_norm=use_norm,\n",
    "    #                exog_only=True)\n",
    "    \n",
    "    model = generic_decoder(enc_dim=enc_dim, output_size=horizon,\n",
    "                    stacks = 8,\n",
    "                    layers = 4,  ## 4 per stack, from the paper\n",
    "                    layer_size = 128,\n",
    "                    exog_block = exog_block,\n",
    "                    use_norm=use_norm)\n",
    "    \n",
    "    train_ds = ts_dataset(timeseries=training_values, static_cat=static_cat,\n",
    "                                    insample_size=input_size,\n",
    "                                    outsample_size=horizon,\n",
    "                                    window_sampling_limit=int(history_size_in_horizons * horizon))\n",
    "\n",
    "    ##\n",
    "    ## batch size?\n",
    "    ##\n",
    "    training_set = DataLoader(train_ds,batch_size=128)#1024)\n",
    "\n",
    "    snapshot_manager = SnapshotManager(snapshot_dir=os.path.join('hub_model_snapshots', model_name),\n",
    "                                        total_iterations=iterations)\n",
    "\n",
    "    model = trainer(snapshot_manager=snapshot_manager,\n",
    "                    model=model,\n",
    "                    training_set=iter(training_set),\n",
    "                    timeseries_frequency=ts_freq,  ## only used by MASE loss fn\n",
    "                    loss_name=loss_fn_name,\n",
    "                    iterations=iterations)\n",
    "    \n",
    "    # Build forecasts\n",
    "    x, x_mask, cat_tensor = train_ds.last_insample_window()\n",
    "    model.eval()\n",
    "    forecast = []\n",
    "    with t.no_grad():\n",
    "        forecast.extend(model(x, x_mask, cat_tensor).cpu().detach().numpy())\n",
    "    \n",
    "    return forecast\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_norm = True\n",
    "var_transform = not use_norm\n",
    "\n",
    "## log transform target?\n",
    "if var_transform:\n",
    "    vals_train[targ_var] = np.log(vals_train[targ_var] + 1.0)\n",
    "    vals_train[\"future_vals\"] = np.log(vals_train[\"future_vals\"] + 1.0)\n",
    "\n",
    "\n",
    "forecasts = {}\n",
    "for i,lookback in enumerate(lookback_opts):\n",
    "    for j,use_static_cat in enumerate(use_cat_opts):\n",
    "        for k,loss_fn_name in enumerate(loss_fn_opts):\n",
    "            model_name = \"model\"+str(i)+str(j)+str(k)\n",
    "            print(\"training \",model_name)\n",
    "            forecasts[model_name] = train_model(model_name,iterations,\n",
    "                                                vals_train,static_cat,\n",
    "                                                lookback,loss_fn_name,use_static_cat,\n",
    "                                                targ_var,exog_vars,\n",
    "                                                horizon,use_norm)\n",
    "            \n",
    "## ensemble using median\n",
    "forecasts[\"median\"] = np.median(np.stack([np.stack(forecasts[k]) for k in forecasts]),axis=0)\n",
    "\n",
    "## forecasts to natural scale\n",
    "if var_transform:\n",
    "    vals_train[targ_var] = np.exp(vals_train[targ_var]) - 1.0\n",
    "    vals_train[\"future_vals\"] = np.exp(vals_train[\"future_vals\"]) - 1.0\n",
    "    for k in forecasts:\n",
    "        forecasts[k] = [np.exp(v) - 1.0 for v in forecasts[k]]\n",
    "\n",
    "upper_fc, lower_fc = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## univariate, without error var\n",
    "\n",
    "def train_univar(model_name, iterations, training_values, lookback, loss_fn_name, use_norm, horizon,\n",
    "                 stacks = 8, layer_size = 128):\n",
    "\n",
    "    horizon = horizon ## forward window size, in time units (weeks)\n",
    "    input_size = lookback * horizon  ## backward window size, in time units\n",
    "    ts_freq = 1 ## only needed for MASE loss\n",
    "    history_size_in_horizons = 60 ## model only uses this much history, max\n",
    "\n",
    "    model = generic(input_size=input_size, output_size=horizon,\n",
    "                    stacks = stacks,\n",
    "                    layers = 4,  ## per stack, from the paper\n",
    "                    layer_size = layer_size,\n",
    "                    use_norm=use_norm, \n",
    "                    dropout=None)\n",
    "    \n",
    "    train_ds = ts_dataset(timeseries=training_values, static_cat=None,\n",
    "                                    insample_size=input_size,\n",
    "                                    outsample_size=horizon,\n",
    "                                    window_sampling_limit=int(history_size_in_horizons * horizon))\n",
    "\n",
    "    training_set = DataLoader(train_ds,batch_size=256)#1024)  ######\n",
    "\n",
    "    snapshot_manager = SnapshotManager(snapshot_dir=os.path.join('hub_model_snapshots', model_name),\n",
    "                                        total_iterations=iterations)\n",
    "\n",
    "    model = trainer(snapshot_manager=snapshot_manager,\n",
    "                    model=model,\n",
    "                    training_set=iter(training_set),\n",
    "                    timeseries_frequency=ts_freq,  ## only used by MASE loss fn\n",
    "                    loss_name=loss_fn_name,\n",
    "                    iterations=iterations)\n",
    "    \n",
    "    # Build forecasts\n",
    "    x, x_mask, cat_tensor = train_ds.last_insample_window()\n",
    "    model.eval()\n",
    "    forecast = []\n",
    "    with t.no_grad():\n",
    "        forecast.extend(model(x, x_mask, cat_tensor).cpu().detach().numpy())\n",
    "    \n",
    "    return forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## univariate, with error var\n",
    "\n",
    "def train_u_var(model_name, iterations, train_mu, lookback, loss_fn_name, use_norm, horizon,\n",
    "                stacks = 8, layer_size = 128):\n",
    "\n",
    "    horizon = horizon ## forward window size, in time units (weeks)\n",
    "    input_size = lookback * horizon  ## backward window size, in time units\n",
    "    history_size_in_horizons = 60 ## model only uses this much history, max\n",
    "\n",
    "    model = generic_var(input_size=input_size, output_size=horizon,\n",
    "                    stacks = stacks,\n",
    "                    layers = 4,  ## per stack, from the paper\n",
    "                    layer_size = layer_size)\n",
    "    \n",
    "    train_ds = ts_dataset(timeseries=train_mu, static_cat=None, insample_size=input_size, outsample_size=horizon,\n",
    "                                    window_sampling_limit=int(history_size_in_horizons * horizon))\n",
    "\n",
    "    training_set = DataLoader(train_ds,batch_size=1024)\n",
    "\n",
    "    snapshot_manager = SnapshotManager(snapshot_dir=os.path.join('hub_model_snapshots', model_name),\n",
    "                                        total_iterations=iterations)\n",
    "\n",
    "    model = trainer_var(snapshot_manager=snapshot_manager,\n",
    "                    model=model,\n",
    "                    training_set=iter(training_set),\n",
    "                    timeseries_frequency=0,  ## not used\n",
    "                    loss_name=loss_fn_name,\n",
    "                    iterations=iterations)\n",
    "    \n",
    "    # Build forecasts\n",
    "    x, x_mask, cat_tensor = train_ds.last_insample_window()\n",
    "    model.eval()\n",
    "    forecast_mu = []\n",
    "    forecast_var = []\n",
    "    with t.no_grad():\n",
    "        f_mu, f_var = model(x, x_mask, cat_tensor)\n",
    "        forecast_mu.extend(f_mu.cpu().detach().numpy())\n",
    "        forecast_var.extend(f_var.cpu().detach().numpy())\n",
    "    \n",
    "    return forecast_mu, forecast_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## weekly data\n",
    "\n",
    "cut = 140\n",
    "\n",
    "vals_train = {}\n",
    "targ_var = \"h_mean\"\n",
    "\n",
    "## load target files\n",
    "df_targ = pd.read_csv(\"storage/training_data/\"+targ_var+\"_weekly.csv\",index_col=0).iloc[:cut,:]\n",
    "vals_train[targ_var] = df_targ.to_numpy(dtype=np.float32).transpose() ## dims are [series, time]\n",
    "\n",
    "df_targ_all = pd.read_csv(\"storage/training_data/\"+targ_var+\"_weekly.csv\",index_col=0)\n",
    "if cut is not None:\n",
    "    test_targets = df_targ_all.iloc[cut:,:].to_numpy(dtype=np.float32).transpose()\n",
    "else:\n",
    "    test_targets = None\n",
    "\n",
    "iterations = 100\n",
    "\n",
    "\n",
    "forecasts = {}\n",
    "lookback_opts = [4,6,8]\n",
    "horizon_opts = [7]\n",
    "loss_fn_opts = [\"MAPE\"]#,\"SMAPE\"]\n",
    "for i,lookback in enumerate(lookback_opts):\n",
    "    for j,horizon in enumerate(horizon_opts):\n",
    "        for k,loss_fn_name in enumerate(loss_fn_opts):\n",
    "            model_name = \"univar_\"+str(i)+str(j)+str(k)\n",
    "            use_windowed_norm = True\n",
    "\n",
    "            if use_windowed_norm:\n",
    "                t_vals = vals_train[targ_var]\n",
    "            else:\n",
    "                u = np.nanmean(vals_train[targ_var],axis=1,keepdims=True)\n",
    "                s = np.nanstd(vals_train[targ_var],axis=1,keepdims=True)\n",
    "                t_vals = (vals_train[targ_var] - u) / s \n",
    "\n",
    "            print(\"training \",model_name)\n",
    "            f_vals = train_univar(model_name,iterations,\n",
    "                                                t_vals,\n",
    "                                                lookback,loss_fn_name,\n",
    "                                                use_windowed_norm,horizon)\n",
    "            if use_windowed_norm:\n",
    "                forecasts[model_name] = f_vals\n",
    "            else:\n",
    "                forecasts[model_name] = u + f_vals * s\n",
    "            \n",
    "## ensemble using median\n",
    "forecasts[\"median\"] = np.median(np.stack([np.stack(forecasts[k]) for k in forecasts]),axis=0)\n",
    "\n",
    "\n",
    "print(\"cut\", cut)\n",
    "for j,horizon in enumerate(horizon_opts):\n",
    "    for i,lookback in enumerate(lookback_opts):\n",
    "        print(\"lookback\",lookback,\"horizon\",horizon,end=\" \")\n",
    "        for k,loss_fn_name in enumerate(loss_fn_opts):\n",
    "            model_name = \"univar_\"+str(i)+str(j)+str(k)\n",
    "            print(calc_loss(forecasts, model_name, test_targets, horizon),end=\" \")\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## daily data\n",
    "\n",
    "cut = None#165 * 7\n",
    "\n",
    "vals_train = {}\n",
    "targ_var = \"h\"\n",
    "\n",
    "df_targ_all = pd.read_csv(\"storage/training_data/\"+targ_var+\"_7ma.csv\",index_col=0)\n",
    "df_targ = df_targ_all.iloc[:cut,:]\n",
    "vals_train[targ_var] = df_targ.to_numpy(dtype=np.float32).transpose() ## dims are [series, time]\n",
    "\n",
    "if cut is not None:\n",
    "    test_targets = df_targ_all.iloc[cut:,:].to_numpy(dtype=np.float32).transpose()\n",
    "else:\n",
    "    test_targets = None\n",
    "\n",
    "iterations = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## without error var\n",
    "\n",
    "iterations = 60\n",
    "\n",
    "lookback_opts = [4,6,8]\n",
    "loss_fn_name = \"MAPE\"\n",
    "horizon = 40 \n",
    "stacks = 12\n",
    "layer_size = 512\n",
    "\n",
    "forecasts = {}\n",
    "for i,lookback in enumerate(lookback_opts):\n",
    "    model_name = \"univar_\"+str(lookback)\n",
    "    print(\"training \",model_name)\n",
    "    forecasts[model_name] = train_univar(model_name,iterations,\n",
    "                                        vals_train[targ_var],\n",
    "                                        lookback,loss_fn_name,\n",
    "                                        True,\n",
    "                                        horizon,stacks,layer_size)\n",
    "\n",
    "## ensemble using median\n",
    "forecasts[\"univar_med\"] = np.median(np.stack([np.stack(forecasts[k]) for k in forecasts]),axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## with error var\n",
    "\n",
    "#iterations = 100\n",
    "\n",
    "var_transform = True\n",
    "\n",
    "## log transform target?\n",
    "if var_transform:\n",
    "    targ_vals = np.log(vals_train[targ_var] + 1.0)\n",
    "    #targ_vals = np.apply_along_axis(lambda s: stats.boxcox(s+1.0)[0], 1, vals_train[targ_var])\n",
    "    #lambdas = np.apply_along_axis(lambda s: stats.boxcox(s+1.0)[1], 1, vals_train[targ_var])\n",
    "else:\n",
    "    targ_vals = vals_train[targ_var]\n",
    "\n",
    "mu_fc={}\n",
    "var_fc={}\n",
    "for i,lookback in enumerate(lookback_opts):\n",
    "    model_name = \"u_var_\"+str(lookback)\n",
    "    print(\"training \",model_name)\n",
    "    mu_fc[model_name], var_fc[model_name] = train_u_var(model_name,iterations,\n",
    "                                                        targ_vals,\n",
    "                                                        lookback,\"\",\n",
    "                                                        False,\n",
    "                                                        horizon,stacks,layer_size)\n",
    "\n",
    "## ensemble using median\n",
    "mu_fc[\"u_var_med\"] = np.median(np.stack([np.stack(mu_fc[k]) for k in mu_fc]),axis=0)\n",
    "var_fc[\"u_var_med\"] = np.median(np.stack([np.stack(var_fc[k]) for k in var_fc]),axis=0)\n",
    "\n",
    "std_fc = {}\n",
    "upper_fc = {}\n",
    "lower_fc = {}\n",
    "for k in mu_fc:\n",
    "    std_fc[k] = [np.sqrt(v) for v in var_fc[k]]\n",
    "    upper_fc[k] = [mu_fc[k][i] + 2.0*v for (i,v) in enumerate(std_fc[k])]\n",
    "    lower_fc[k] = [mu_fc[k][i] - 2.0*v for (i,v) in enumerate(std_fc[k])]\n",
    "\n",
    "\n",
    "## forecasts to natural scale\n",
    "if var_transform:\n",
    "    for k in mu_fc:\n",
    "        mu_fc[k] = [np.exp(v) - 1.0 for v in mu_fc[k]]\n",
    "        upper_fc[k] = [np.exp(v) - 1.0 for v in upper_fc[k]]    \n",
    "        lower_fc[k] = [np.exp(v) - 1.0 for v in lower_fc[k]]    \n",
    "        #mu_fc[k] = [inv_boxcox(v,lambdas[i]) - 1.0 for (i,v) in enumerate(mu_fc[k])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "forecasts = forecasts | mu_fc\n",
    "\n",
    "print(\"cut\", cut)\n",
    "for i,lookback in enumerate([*lookback_opts, \"med\"]):\n",
    "    print(\"lookback\",lookback,end=\" \")\n",
    "    for model_name in [\"univar_\"+str(lookback), \"u_var_\"+str(lookback)]:\n",
    "        print(calc_loss(forecasts, model_name, test_targets, horizon),end=\" \")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_manager = SnapshotManager(snapshot_dir=os.path.join('hub_model_snapshots', \"univar_100\"), total_iterations=iterations)\n",
    "\n",
    "ldf = snapshot_manager.load_training_losses()\n",
    "_, ax = plt.subplots(figsize=[4,3])\n",
    "ax.plot(ldf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotpred(forecasts, \"u_var_med\", 20, vals_train[targ_var], test_targets, horizon, lower_fc, upper_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotpred(forecasts, \"univar_000\", 4, vals_train[targ_var], test_targets, horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
