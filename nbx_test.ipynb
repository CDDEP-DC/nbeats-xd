{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ***\n",
    "## NOTE: edit default_device() in common/torch/ops.py for your gpu\n",
    "## ***\n",
    "###\n",
    "\n",
    "## versions:\n",
    "## Python    : 3.11.5\n",
    "## numpy     : 1.26.0\n",
    "## torch     : 2.1.0\n",
    "## pandas    : 2.1.1\n",
    "\n",
    "# licensed under the Creative Commons - Attribution-NonCommercial 4.0\n",
    "# International license (CC BY-NC 4.0):\n",
    "# https://creativecommons.org/licenses/by-nc/4.0/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import shutil\n",
    "import datetime\n",
    "import requests\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy import stats\n",
    "\n",
    "from common.sampler import ts_dataset\n",
    "from common.torch.snapshots import SnapshotManager\n",
    "from experiments.trainer import trainer_var\n",
    "from experiments.model import generic_dec_var\n",
    "from models.exog import LSTM_test, TCN_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## not the actual loss fn, just for comparing different forecasts\n",
    "def calc_loss(forecasts, name, test_targets, horizon):\n",
    "    #lfn = t.nn.functional.l1_loss\n",
    "    #def lfn(a,b):\n",
    "    #    x = ((a-b)/b)\n",
    "    #    return x.abs().nanmean()\n",
    "    def lfn(a,b):\n",
    "        return 2.0 * t.nn.functional.l1_loss(a,b) / (a.nanmean() + b.nanmean())\n",
    "    return lfn(t.tensor(np.array(forecasts[name],dtype=np.float32))[:,:horizon], \n",
    "               t.tensor(np.array(test_targets,dtype=np.float32))[:,:horizon])\n",
    "\n",
    "def plotpred(forecasts, name, ser, training_targets, test_targets, horizon, lower_fc=None, upper_fc=None, x_start = 0, date0 = \"2020-07-14\"):\n",
    "    x_end = training_targets.shape[1]\n",
    "    dates=pd.date_range(pd.to_datetime(date0),periods=x_end+horizon,freq=\"D\")\n",
    "    colors = [\"black\",\"orangered\"]\n",
    "    #colors = [\"white\",\"yellow\"]\n",
    "    _, ax = plt.subplots(figsize=(7,5))\n",
    "    ax.grid(alpha=0.2)\n",
    "    pd.Series(training_targets[ser,x_start:x_end],index=dates[x_start:x_end]).plot(ax=ax,grid=True,color=colors[0],linewidth=0.5)\n",
    "    if test_targets is not None:\n",
    "        test_end = min(test_targets.shape[1], horizon)\n",
    "        pd.Series(test_targets[ser,0:test_end],index=dates[x_end:x_end+test_end]).plot(ax=ax,grid=True,color=colors[0],linewidth=0.5)\n",
    "    pd.Series(forecasts[name][ser],index=dates[x_end:x_end+horizon]).plot(ax=ax,grid=True,color=colors[1],linewidth=1.5,alpha=0.8)\n",
    "    if upper_fc is not None:\n",
    "        ax.fill_between(dates[x_end:x_end+horizon],lower_fc[name][ser],upper_fc[name][ser],color=colors[1],alpha=0.4)\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this fn returns a function suitable for using in a loop to generate an ensemble\n",
    "\n",
    "def training_fn(training_data: Dict[str, np.ndarray], \n",
    "                static_categories: np.ndarray, \n",
    "                target_key: str,\n",
    "                horizon: int, ## forecast horizon\n",
    "                windowed_norm: bool, ## whether the model should normalize the target data by sample window\n",
    "                init_LR = 0.001, ## initial learning rate; default from nbeats; should probably use lower\n",
    "                batch_size = 1024): ## default batch size from nbeats; should probably use lower\n",
    "\n",
    "    enc_temporal = True ## generate an encoding that preserves temporal structure\n",
    "    static_cat_embed_dim = 3 ## dimension of vector embedding if using static_categories\n",
    "    history_size_in_horizons = 60 ## maximum length of history to consider for training, in # of horizons\n",
    "\n",
    "    ## used in the fn below\n",
    "    def generate_forecast(m, data_iterator):\n",
    "        x, x_mask, cat_tensor = data_iterator.last_insample_window()\n",
    "        m.eval()\n",
    "        with t.no_grad():\n",
    "            f_mu, f_var = m(x, x_mask, cat_tensor)\n",
    "        return f_mu.cpu().detach().numpy(), f_var.cpu().detach().numpy()\n",
    "\n",
    "    ## result fn returns forecast; model is saved in snapshots folder\n",
    "    def ret_fn(model_name: str, \n",
    "                iterations: int, \n",
    "                lookback: int, ## backwards window size, in # of horizons\n",
    "                use_exog_vars: List[str], \n",
    "                use_static_cat: bool, ## if true, static category will be transformed into a vector embedding\n",
    "                loss_fn_name: str,\n",
    "                nbeats_stacks: int = 8, ## number of layer stacks; more data can support a deeper model\n",
    "                nbeats_hidden_dim: int = 512, ## longer input sequence needs larger hidden dimension\n",
    "                nbeats_dropout: Optional[float] = None,\n",
    "                encoder_k: int = 3,\n",
    "                encoder_n: Optional[int] = None, ## calculated below if missing\n",
    "                encoder_hidden_dim: int = 128,\n",
    "                encoder_dropout: float = 0.2, ## default for TCN from Bai et al\n",
    "                force_positive_forecast: bool = False): ## if loss fn requires > 0\n",
    "\n",
    "        input_size = lookback * horizon  ## backward window size, in time units\n",
    "        use_vars = [target_key, *use_exog_vars]\n",
    "        ## training data dims are [series, time, variables]\n",
    "        ## where the first variable is the target, and the rest are covariates\n",
    "        training_values = np.stack(\n",
    "                            [training_data[k] for k in use_vars]\n",
    "                            ).transpose([1,2,0])\n",
    "        \n",
    "        n_features = training_values.shape[2] - 1 ## number of exogenous covariates\n",
    "        n_embed = static_categories.shape[0] if use_static_cat else 0\n",
    "        embed_dim = static_cat_embed_dim if use_static_cat else 0\n",
    "\n",
    "        if encoder_n is None: ## use minimum needed to cover input size\n",
    "            ## TCN receptive field size = 2(k-1)(2^n - 1)+1 where k is kernel size and n is # blocks\n",
    "            encoder_n = int(np.ceil(np.log2(1.0 + (0.5*(input_size - 1.0) / (encoder_k - 1.0)))))\n",
    "\n",
    "        exog_block = TCN_encoder(n_features, [encoder_hidden_dim]*encoder_n, encoder_k, encoder_dropout, enc_temporal, n_embed, embed_dim)\n",
    "        ## same idea, but using LSTM; TCN encoder seems to work slightly better (but has more moving parts)\n",
    "        #exog_block = LSTM_test(n_features=n_features,input_size=input_size,output_size=horizon,layer_size=-1,n_embed=n_embed,embed_dim=embed_dim,decoder_extra_layers=0,\n",
    "        #            lstm_layers=1,lstm_hidden=enc_hid,temporal=enc_temporal,decode=False)\n",
    "\n",
    "        enc_dim = input_size if enc_temporal else encoder_hidden_dim\n",
    "        ## constructs the model; defined in experiments/model.py\n",
    "        model = generic_dec_var(enc_dim=enc_dim, output_size=horizon,\n",
    "                        stacks = nbeats_stacks,\n",
    "                        layers = 4,  ## 4 per stack, from the nbeats paper\n",
    "                        layer_size = nbeats_hidden_dim,\n",
    "                        exog_block = exog_block,\n",
    "                        use_norm=windowed_norm,\n",
    "                        dropout=nbeats_dropout,\n",
    "                        force_positive=force_positive_forecast)\n",
    "        \n",
    "        ## dataset iterator; defined in common/sampler.py\n",
    "        train_ds = ts_dataset(timeseries=training_values, static_cat=static_categories,\n",
    "                                        insample_size=input_size,\n",
    "                                        outsample_size=horizon,\n",
    "                                        window_sampling_limit=int(history_size_in_horizons * horizon))\n",
    "\n",
    "        training_set = DataLoader(train_ds,batch_size=batch_size)\n",
    "\n",
    "        snapshot_manager = SnapshotManager(snapshot_dir=os.path.join('hub_model_snapshots', model_name),\n",
    "                                            total_iterations=iterations)\n",
    "\n",
    "        ## training loop, including loss fn; defined in experiments/trainer.py\n",
    "        model = trainer_var(snapshot_manager=snapshot_manager,\n",
    "                        model=model,\n",
    "                        training_set=iter(training_set),\n",
    "                        timeseries_frequency=0,  ## not used\n",
    "                        loss_name=loss_fn_name,\n",
    "                        iterations=iterations,\n",
    "                        learning_rate=init_LR)\n",
    "        \n",
    "        # training done; generate forecasts\n",
    "        return generate_forecast(model, train_ds)\n",
    "\n",
    "    return ret_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = \"https://media.githubusercontent.com/media/reichlab/covid19-forecast-hub/master/data-truth/truth-Incident%20Hospitalizations.csv\"\n",
    "#df = pd.read_csv(f,dtype={\"location\":str})\n",
    "#d_str = pd.to_datetime(df.date).max().strftime(\"%m-%d\")\n",
    "#df.to_csv(\"storage/truth-inc-hosp-\" + d_str + \".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the latest training data\n",
    "\n",
    "#from data_utils import download_training_data\n",
    "#download_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## delete saved models\n",
    "\n",
    "#try:\n",
    "#    shutil.rmtree(\"hub_model_snapshots\")\n",
    "#except:\n",
    "#    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## if using mac metal:\n",
    "t.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## settings\n",
    "\n",
    "## number of training epochs; too many (relative to learning rate) overfits\n",
    "iterations = 400\n",
    "init_LR = 0.00025 #0.0001 # learning rate; lower with more iterations seems to work better\n",
    "## batch size?\n",
    "## nbeats is ok with large batch size, but it seems to hurt TCN\n",
    "## the two parts probably learn at different rates; not sure how to handle that\n",
    "batch_size = 256 #128 #1024 #\n",
    "\n",
    "## ensemble using these options:\n",
    "lookback_opts = [3,4,4,5,5,6,7]#[3,3,4,4,5,5,6,6,7,7]#  ## backward window size, in horizons\n",
    "use_cat_opts = [False] ## whether to use static_cat; current implementation always makes forecasts worse\n",
    "\n",
    "## forecast horizon (in time units)\n",
    "horizon = 40 #6\n",
    "\n",
    "## weekly data vs daily \n",
    "##  (\"7ma\" = 7 day moving average -- mean of previous week)\n",
    "##  (\"3ma\" = 3 day centered MA -- intended to smoothe out reporting errors without losing too much variance)\n",
    "data_suffix = \"3ma\" #\"7ma\" #\"weekly\" #\"unsmoothed\" #\n",
    "targ_var = \"h_mean\" if data_suffix==\"weekly\" else \"h\" #\"h_log\" # \n",
    "## loss function (defined in experiments/trainer.py)\n",
    "lfn_name = \"t_nll\" # \"norm_nll\" if targ_var==\"h_log\" else \"t_nll\" #\n",
    "force_positive_forecast = False ## if loss fn requires > 0\n",
    "\n",
    "normalize_target = False ## normalize the target var before passing to model? (see notes below)\n",
    "use_windowed_norm = True ## normalize inside the model by window? (tends to improve forecasts; ref. Smyl 2020)\n",
    "\n",
    "## which covariates to include (including useless ones hinders learning)\n",
    "exog_vars = [\"doy\",\"dewpC\",\"vacc_rate\"]\n",
    "#exog_vars = [\"t\",\"doy\",\"dewpC\",\"vacc_rate\"]\n",
    "#exog_vars = [\"t\",\"dewpC\",\"vacc_rate\"]\n",
    "#exog_vars = [\"tsa_by_pop\",\"tempC\",\"dewpC\",\"t\",\"t_voc\",\"vacc_rate\",\"pop_density_2020\",\"med_age_2023\"]\n",
    "#exog_vars = [\"t\",\"future_vals\"]\n",
    "\n",
    "nbeats_stacks=12 #8 # more data can support deeper model\n",
    "nbeats_hidden_dim=512 #128 ## should be larger than length of lookback window\n",
    "nbeats_dropout=0.2 ## could help prevent overfitting? default = None\n",
    "encoder_k = 4 #3\n",
    "#encoder_n = 5 #4 ## TCN receptive field size = 2(k-1)(2^n - 1)+1\n",
    "encoder_hidden_dim=128\n",
    "encoder_dropout=0.2 ## default is 0.2\n",
    "\n",
    "## if we're using weekly or 7-day moving average to forecast daily values,\n",
    "## the daily variance is 7 * that of the weekly means or smoothed data\n",
    "## (set this to 1.0 if we used actual daily data, or if we want confidence intervals for weekly means)\n",
    "##  (also: this relationship breaks down for log-transformed data)\n",
    "variance_scale = 7.0 if (data_suffix==\"weekly\" or data_suffix==\"7ma\") else 1.0\n",
    "data_is_log = (targ_var==\"h_log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training data cutoff index (set to None to train with all available data)\n",
    "cut = 908 #1271 #1238 #165 * 7 #None\n",
    "forecast_delay = 2 # 10 # None ## for hub csv output, see below\n",
    "\n",
    "## change settings\n",
    "lfn_name = \"gamma_nll\"\n",
    "force_positive_forecast = True\n",
    "lookback_opts = [3,4,5]\n",
    "nbeats_stacks=8\n",
    "\n",
    "#test_cut_vals = [1264, 1208, 1152, 1145, 1096] + [950, 908, 901, 740, 733, 642]\n",
    "#forecast_delay_days = [10, 10, 10, 10, 10] + [2, 2, 2, 2, 2, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load and normalize training data\n",
    "\n",
    "df_targ_all = pd.read_csv(\"storage/training_data/\"+targ_var+\"_\"+data_suffix+\".csv\",index_col=0)\n",
    "\n",
    "df_targ = df_targ_all.iloc[:cut,:]\n",
    "data_index = df_targ.index\n",
    "data_columns = df_targ.columns\n",
    "\n",
    "vals_train = {}\n",
    "vals_train[targ_var] = df_targ.to_numpy(dtype=np.float32).transpose() ## dims are [series, time]\n",
    "\n",
    "if cut is not None:\n",
    "    test_targets = df_targ_all.iloc[cut:,:].to_numpy(dtype=np.float32).transpose()\n",
    "else:\n",
    "    test_targets = None\n",
    "\n",
    "if data_is_log: # used log data\n",
    "    vals_train[\"nat_scale\"] = np.exp(vals_train[targ_var]) - 1.0\n",
    "    test_targets = np.exp(test_targets) - 1.0 if test_targets is not None else None\n",
    "else:\n",
    "    vals_train[\"nat_scale\"] = vals_train[targ_var]\n",
    "\n",
    "## for testing, include the actual future values as a predictor\n",
    "#fut_lag = horizon * 2\n",
    "#vals_train[\"future_vals\"] = df_targ_all.iloc[fut_lag:fut_lag+cut,:].to_numpy(dtype=np.float32).transpose()\n",
    "#vals_train[\"log_future\"] = np.log(vals_train[\"future_vals\"] + 1.0)\n",
    "\n",
    "## use 7-day MA for daily weather data\n",
    "covar_suffix = \"weekly\" if data_suffix==\"weekly\" else \"7ma\"\n",
    "## load covar files\n",
    "load_co_vars = [\"tempC\",\"dewpC\"]\n",
    "for f in load_co_vars:\n",
    "    df = pd.read_csv(\"storage/training_data/\"+f+\"_\"+covar_suffix+\".csv\",index_col=0).iloc[:cut,:]\n",
    "    assert df.index.equals(data_index), \"data index mismatch\"\n",
    "    assert df.columns.equals(data_columns), \"data columns mismatch\"\n",
    "    vals_train[f] = df.to_numpy(dtype=np.float32).transpose() ## dims are [series, time]\n",
    "\n",
    "## for weather, normalize using overall mean instead of by series? (otherwise losing info)\n",
    "for k in [\"tempC\",\"dewpC\",\"AH\"]:\n",
    "    if k in vals_train:\n",
    "        vals_train[k] = (vals_train[k] - np.nanmean(vals_train[k])) / np.nanstd(vals_train[k])\n",
    "\n",
    "## include travel data?\n",
    "read_tsa_data = False\n",
    "if read_tsa_data:\n",
    "    travel_file = \"tsa_by_pop_weekly\" if data_suffix==\"weekly\" else \"tsa_by_pop_daily\"\n",
    "    df = pd.read_csv(\"storage/training_data/\"+travel_file+\".csv\",index_col=0).iloc[:cut,:]\n",
    "    assert df.index.equals(data_index), \"data index mismatch\"\n",
    "    assert df.columns.equals(data_columns), \"data columns mismatch\"\n",
    "    vals_train[\"tsa_by_pop\"] = df.to_numpy(dtype=np.float32).transpose()\n",
    "\n",
    "for k in [\"tsa_by_pop\"]:\n",
    "    if k in vals_train:\n",
    "        ## normalize by series, losing pop size info but preserving relative change?\n",
    "        u = np.nanmean(vals_train[k],axis=1,keepdims=True)\n",
    "        vals_train[k] = vals_train[k] / u\n",
    "        # or normalize by global mean?\n",
    "        #vals_train[k] = vals_train[k] / np.nanmean(vals_train[k])\n",
    "\n",
    "## add time as predictor; same scale as other predictors\n",
    "vals_train[\"t\"] = np.tile(np.linspace(-2,2,vals_train[targ_var].shape[1],dtype=np.float32), (vals_train[targ_var].shape[0],1))\n",
    "\n",
    "## also day of year\n",
    "vals_train[\"doy\"] = np.tile(np.array(-2.0 + 4.0 * pd.to_datetime(data_index).dayofyear.values / 366.0, dtype=np.float32), \n",
    "            (vals_train[targ_var].shape[0],1))\n",
    "\n",
    "## add predictor: time since last variant of concern (dates from cdc website)\n",
    "## first wk of available target data = 7/14-7/20 (19-20 weeks from 3/1)\n",
    "## a/b/g: 12/29/20  (24 weeks from 7/14)\n",
    "## e: 3/19/21  (wk 35)\n",
    "## d: 6/15/21  (wk 48)\n",
    "## o: 11/26/21  (71)\n",
    "data_start = pd.to_datetime(df_targ.index[0])\n",
    "time_a = pd.to_datetime(\"2020-12-29\") \n",
    "time_e = pd.to_datetime(\"2021-03-19\")\n",
    "time_d = pd.to_datetime(\"2021-06-15\")\n",
    "time_o = pd.to_datetime(\"2021-11-26\")\n",
    "time_unit = 7 if data_suffix==\"weekly\" else 1\n",
    "delta_a = (time_a - data_start).days // time_unit\n",
    "delta_e = (time_e - data_start).days // time_unit\n",
    "delta_d = (time_d - data_start).days // time_unit\n",
    "delta_o = (time_o - data_start).days // time_unit\n",
    "data_offset = (data_start - pd.to_datetime(\"2020-03-01\")).days // time_unit\n",
    "\n",
    "timepoints = np.arange(vals_train[targ_var].shape[1])\n",
    "t_a = timepoints - delta_a; t_a[t_a < 0] = 99999\n",
    "t_e = timepoints - delta_e; t_e[t_e < 0] = 99999\n",
    "t_d = timepoints - delta_d; t_d[t_d < 0] = 99999\n",
    "t_o = timepoints - delta_o; t_o[t_o < 0] = 99999\n",
    "time_since_voc = np.stack([timepoints+data_offset,t_a,t_e,t_d,t_o]).min(axis=0)\n",
    "time_since_voc = (2.0 * time_since_voc / np.max(time_since_voc))\n",
    "vals_train[\"t_voc\"] = np.tile(time_since_voc, (vals_train[targ_var].shape[0],1))\n",
    "\n",
    "## static predictors\n",
    "load_static_real = [\"pop_density_2020\",\"med_age_2023\"]\n",
    "\n",
    "for f in load_static_real:\n",
    "    df = pd.read_csv(\"storage/training_data/\"+f+\".csv\",dtype={\"fips\":str}).set_index(\"fips\").sort_index()\n",
    "    assert df.index.equals(data_columns), \"static data mismatch\"\n",
    "    ## repeat the same value across time steps\n",
    "    vals_train[f] = np.tile(df.to_numpy(dtype=np.float32),(1,vals_train[targ_var].shape[1]))\n",
    "\n",
    "## log-transform, then normalize using z score\n",
    "for k in [\"pop_density_2020\"]:\n",
    "    if k in vals_train:\n",
    "        log_v = np.log(vals_train[k])\n",
    "        ## z score across series (axis 0)\n",
    "        u = np.nanmean(log_v,axis=0,keepdims=True)\n",
    "        s = np.nanstd(log_v,axis=0,keepdims=True)\n",
    "        vals_train[k] = (log_v - u) / s\n",
    "\n",
    "## normalize using z score\n",
    "for k in [\"med_age_2023\"]:\n",
    "    if k in vals_train:\n",
    "        ## z score across series (axis 0)\n",
    "        u = np.nanmean(vals_train[k],axis=0,keepdims=True)\n",
    "        s = np.nanstd(vals_train[k],axis=0,keepdims=True)\n",
    "        vals_train[k] = (vals_train[k] - u) / s\n",
    "\n",
    "## vaccination data\n",
    "df = pd.read_csv(\"storage/training_data/vacc_full_pct_to_may23.csv\",index_col=0)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "assert df.columns.equals(data_columns), \"data columns mismatch\"\n",
    "## merge in estimated vaccination rates at the dates indexed in the target data\n",
    "## vaccination data is only avail to 2023-05-10, so assume it's constant after that, using ffill()\n",
    "## then fill data before 2021-01-12 with 0's\n",
    "vacc_pct_fake = pd.DataFrame(index=pd.to_datetime(df_targ.index)).join(df).ffill().fillna(0.0)\n",
    "vals_train[\"vacc_rate\"] = vacc_pct_fake.to_numpy(dtype=np.float32).transpose()\n",
    "## normalize by global max? (by series doesn't make sense)\n",
    "vals_train[\"vacc_rate\"] = 2.0 * vals_train[\"vacc_rate\"] / np.nanmax(vals_train[\"vacc_rate\"])\n",
    "\n",
    "## just one static categorical covariate for now, identifying which time series each window comes from\n",
    "## categorical vars should either be one-hot encoded, or converted to a learned \"embedding\" vector\n",
    "static_cat = np.arange(vals_train[targ_var].shape[0],dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## optionally normalize the target\n",
    "if normalize_target:\n",
    "    ## (note: don't log transform the processed data; load log data instead)\n",
    "    ## try transforming series to common scale\n",
    "    ## NOTE: in the unscaled data, series with small values contribute less to the weight gradients\n",
    "    ##  scaling makes the model learn better from states with small populations, whose data is noisier and more error prone\n",
    "    ##  this could make the overall forecast worse; can maybe be compensated with more training iterations\n",
    "    inv_scale = np.nanmedian(vals_train[targ_var], axis=1, keepdims=True)\n",
    "    target_key = \"scaled_\" + targ_var\n",
    "    vals_train[target_key] = vals_train[targ_var] / inv_scale\n",
    "else:\n",
    "    inv_scale = np.ones((vals_train[targ_var].shape[0],1))\n",
    "    target_key = targ_var\n",
    "\n",
    "\n",
    "## create training function\n",
    "train_m_var = training_fn(training_data=vals_train,\n",
    "                          static_categories=static_cat,\n",
    "                          target_key=target_key,\n",
    "                          horizon=horizon,\n",
    "                          windowed_norm=use_windowed_norm,\n",
    "                          init_LR=init_LR,\n",
    "                          batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(iterations, data_suffix, target_key, lfn_name, force_positive_forecast, variance_scale, \n",
    " data_is_log, normalize_target, horizon, use_windowed_norm,\n",
    " exog_vars, [np.isnan(vals_train[k]).sum() for k in exog_vars], [vals_train[k].shape for k in exog_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_fc={}\n",
    "var_fc={}\n",
    "for i,lookback in enumerate(lookback_opts):\n",
    "    for j,use_static_cat in enumerate(use_cat_opts):\n",
    "        model_name = lfn_name+\"_\"+str(i)+\"_\"+str(int(use_static_cat))\n",
    "        model_name = model_name+\"_\"+str(cut) if cut is not None else model_name\n",
    "        print(\"training \",model_name)\n",
    "        mu_fc[model_name], var_fc[model_name] = train_m_var(model_name=model_name,\n",
    "                                                            iterations=iterations,\n",
    "                                                            lookback=lookback,\n",
    "                                                            use_exog_vars=exog_vars,\n",
    "                                                            use_static_cat=use_static_cat,\n",
    "                                                            loss_fn_name=lfn_name,\n",
    "                                                            nbeats_stacks=nbeats_stacks,\n",
    "                                                            nbeats_hidden_dim=nbeats_hidden_dim,\n",
    "                                                            nbeats_dropout=nbeats_dropout,\n",
    "                                                            encoder_k=encoder_k,\n",
    "                                                            #encoder_n=None, ## auto calculated\n",
    "                                                            encoder_hidden_dim=encoder_hidden_dim,\n",
    "                                                            encoder_dropout=encoder_dropout,\n",
    "                                                            force_positive_forecast=force_positive_forecast) \n",
    "\n",
    "## forecast shape for each model is [series, time]\n",
    "## ensemble using median across models\n",
    "mu_fc[\"median\"] = np.median(np.stack([mu_fc[k] for k in mu_fc]),axis=0)\n",
    "var_fc[\"median\"] = np.median(np.stack([var_fc[k] for k in var_fc]),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert forecasts to natural scale and generate CIs/quantiles\n",
    "\n",
    "forecasts = {}\n",
    "upper_fc = {}\n",
    "lower_fc = {}\n",
    "for k in mu_fc:\n",
    "    mu = mu_fc[k].copy()\n",
    "    s = np.sqrt(var_fc[k] * variance_scale)\n",
    "    if normalize_target:\n",
    "        mu = mu * inv_scale\n",
    "        s = s * inv_scale\n",
    "    if data_is_log: ## convert forecasts to natural scale\n",
    "        forecasts[k] = np.exp(mu) - 1.0\n",
    "        upper_fc[k] = np.exp(mu + 2.0*s) - 1.0\n",
    "        lower_fc[k] = np.exp(mu - 2.0*s) - 1.0\n",
    "    else:\n",
    "        forecasts[k] = mu\n",
    "        upper_fc[k] = mu + 2.0*s\n",
    "        lower_fc[k] = mu - 2.0*s\n",
    "\n",
    "quantile_fc = {}\n",
    "## quantiles requested by forecast hub\n",
    "qtiles = [0.01, 0.025, *np.linspace(0.05, 0.95, 19).round(2), 0.975, 0.99]\n",
    "\n",
    "if lfn_name == \"t_nll\": ## the model used a t-distributed error variance with df=5\n",
    "    Z = [stats.t.ppf(q=x,df=5) for x in qtiles]\n",
    "else:\n",
    "    Z = [stats.norm.ppf(q=x) for x in qtiles]\n",
    "\n",
    "for k in mu_fc:\n",
    "    mu = mu_fc[k].copy()\n",
    "    s = np.sqrt(var_fc[k] * variance_scale)\n",
    "    if normalize_target:\n",
    "        mu = mu * inv_scale\n",
    "        s = s * inv_scale\n",
    "    if data_is_log: ## convert quantiles to natural scale\n",
    "        V = [np.exp(mu + x*s)-1.0 for x in Z]\n",
    "    else:\n",
    "        V = [mu + x*s for x in Z]\n",
    "    quantile_fc[k] = np.stack(V,axis=2) ## [series, time, quantiles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtest = {}\n",
    "ftest = {}\n",
    "utest = {}\n",
    "ltest = {}\n",
    "for k in mu_fc:\n",
    "    mu = mu_fc[k].copy()\n",
    "    s2 = var_fc[k] * variance_scale\n",
    "    if normalize_target:\n",
    "        mu = mu * inv_scale\n",
    "        s2 = s2 * inv_scale * inv_scale \n",
    "    s = np.sqrt(s2)\n",
    "    #if lfn_name == \"t_nll\":\n",
    "    #    V = [stats.t.ppf(q=x,loc=mu,scale=s,df=5) for x in qtiles]\n",
    "    #    ftest[k],utest[k],ltest[k] = [stats.t.ppf(q=x,loc=mu,scale=s,df=5) for x in [0.5,0.975,0.025]]\n",
    "    #elif lfn_name == \"norm_nll\":\n",
    "    #    V = [stats.norm.ppf(q=x,loc=mu,scale=s) for x in qtiles]\n",
    "    #    ftest[k],utest[k],ltest[k] = [stats.norm.ppf(q=x,loc=mu,scale=s) for x in [0.5,0.975,0.025]]\n",
    "    #elif lfn_name == \"gamma_nll\":\n",
    "    ##\n",
    "    ## try using gamma err dist, even if trained on a different error fn?\n",
    "    ##\n",
    "    V = [stats.gamma.ppf(q=x, a=(mu*mu/s2) , scale=(s2/mu)) for x in qtiles]\n",
    "    ftest[k],utest[k],ltest[k] = [stats.gamma.ppf(q=x, a=(mu*mu/s2) , scale=(s2/mu)) for x in [0.5,0.975,0.025]]\n",
    "    qtest[k] = np.stack(V,axis=2) ## [series, time, quantiles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_manager = SnapshotManager(snapshot_dir=os.path.join('hub_model_snapshots', [k for k in mu_fc][2]), total_iterations=iterations)\n",
    "\n",
    "ldf = snapshot_manager.load_training_losses()\n",
    "_, ax = plt.subplots(figsize=[4,3])\n",
    "ax.plot(ldf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in forecasts:\n",
    "    print(k, calc_loss(forecasts, k, test_targets, horizon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = \"median\"\n",
    "#plotpred(forecasts, \"median\", 20, vals_train[\"nat_scale\"], test_targets, horizon, lower_fc, upper_fc, cut-400)\n",
    "#plt.savefig(\"storage/\"+lfn_name+\"_MD_\"+str(cut)+\".png\")\n",
    "plotpred(ftest, k, 20, vals_train[\"nat_scale\"], test_targets, horizon, ltest, utest, cut-400)\n",
    "#plt.savefig(\"storage/gamma_MD_\"+str(cut)+\".png\")\n",
    "#plotpred(forecasts, \"median\", 4, vals_train[\"nat_scale\"], test_targets, horizon, lower_fc, upper_fc, cut-400)\n",
    "#plt.savefig(\"storage/\"+lfn_name+\"_CA_\"+str(cut)+\".png\")\n",
    "plotpred(ftest, k, 4, vals_train[\"nat_scale\"], test_targets, horizon, ltest, utest, cut-400)\n",
    "#plt.savefig(\"storage/gamma_CA_\"+str(cut)+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_forecast = {}\n",
    "us_upper = {}\n",
    "us_lower = {}\n",
    "\n",
    "## estimated correlations, to calculate var(sum)\n",
    "##  (because var(sum) = sum(var) only for independent variables)\n",
    "corr_mat = np.corrcoef(vals_train[\"nat_scale\"])  \n",
    "\n",
    "for k in forecasts:\n",
    "    us_forecast[k] = forecasts[k].sum(axis=0,keepdims=True) ## sum mu on natural scale\n",
    "\n",
    "    ## covar_i_j = correlation_i_j * std_i * std_j\n",
    "    st_devs = np.sqrt(var_fc[k] * variance_scale) * inv_scale\n",
    "    ## var(sum) = sum(covar) at each timepoint for which variance was forecast\n",
    "    est_varsums = np.array([np.nansum(corr_mat * st_devs[:,i,None] * st_devs[:,i]) for i in range(st_devs.shape[1])])\n",
    "\n",
    "    if data_is_log: ## variances are on log scale\n",
    "        ## summing var(log) gives nonsense results; convert var(log) to approx var(natural), then sum?\n",
    "        #approx_var = ( np.square(forecasts[k]) * var_fc[k] ).sum(axis=0,keepdims=True) \n",
    "        #approx_var = ( np.square(forecasts[k]) * (-1.0+np.exp(var_fc[k])) ).sum(axis=0,keepdims=True) \n",
    "        #s = np.sqrt(approx_var) * inv_scale\n",
    "        #that doesn't look right either so...\n",
    "        us_upper[k] = upper_fc[k].sum(axis=0,keepdims=True)\n",
    "        us_lower[k] = lower_fc[k].sum(axis=0,keepdims=True)\n",
    "    else: ## variances are on natural scale\n",
    "        ## using estimated var(sum):\n",
    "        s = np.sqrt(est_varsums)\n",
    "        us_upper[k] = us_forecast[k] + 2.0*s\n",
    "        us_lower[k] = us_forecast[k] - 2.0*s\n",
    "        #if that doesn't work...\n",
    "        #us_upper[k] = upper_fc[k].sum(axis=0,keepdims=True)\n",
    "        #us_lower[k] = lower_fc[k].sum(axis=0,keepdims=True)\n",
    "        \n",
    "\n",
    "us_quantile = {}\n",
    "for k in quantile_fc:\n",
    "\n",
    "    ## covar_i_j = correlation_i_j * std_i * std_j\n",
    "    st_devs = np.sqrt(var_fc[k] * variance_scale) * inv_scale\n",
    "    ## var(sum) = sum(covar) at each timepoint for which variance was forecast\n",
    "    est_varsums = np.array([np.nansum(corr_mat * st_devs[:,i,None] * st_devs[:,i]) for i in range(st_devs.shape[1])])\n",
    "\n",
    "    if data_is_log:\n",
    "        ## this isn't right, but it looks ok and computing var(sum) from var(logs) is tricky\n",
    "        us_quantile[k] = quantile_fc[k].sum(axis=0,keepdims=True)\n",
    "    else:\n",
    "        ## using estimated var(sum):\n",
    "        s = np.sqrt(est_varsums)\n",
    "        V = [us_forecast[k] + x*s for x in Z]\n",
    "        us_quantile[k] = np.stack(V,axis=2) ## [series, time, quantiles]\n",
    "        #if that doesn't work...\n",
    "        #us_quantile[k] = quantile_fc[k].sum(axis=0,keepdims=True)\n",
    "\n",
    "us_train = vals_train[\"nat_scale\"].sum(axis=0,keepdims=True)\n",
    "us_test = test_targets.sum(axis=0,keepdims=True) if test_targets is not None else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_qtest = {}\n",
    "us_ftest = {}\n",
    "us_utest = {}\n",
    "us_ltest = {}\n",
    "\n",
    "## estimated correlations, to calculate var(sum)\n",
    "##  (because var(sum) = sum(var) only for independent variables)\n",
    "corr_mat = np.corrcoef(vals_train[\"nat_scale\"])  \n",
    "\n",
    "for k in mu_fc:\n",
    "    mu = (mu_fc[k] * inv_scale).sum(axis=0,keepdims=True)  ## sum(natural scale mu)\n",
    "\n",
    "    ## covar_i_j = correlation_i_j * std_i * std_j\n",
    "    st_devs = np.sqrt(var_fc[k] * variance_scale) * inv_scale\n",
    "    ## var(sum) = sum(covar) at each timepoint for which variance was forecast\n",
    "    s2 = np.array([np.nansum(corr_mat * st_devs[:,i,None] * st_devs[:,i]) for i in range(st_devs.shape[1])])\n",
    "    s = np.sqrt(s2)\n",
    "    #if lfn_name == \"t_nll\":\n",
    "    #    V = [stats.t.ppf(q=x,loc=mu,scale=s,df=5) for x in qtiles]\n",
    "    #    us_ftest[k],us_utest[k],us_ltest[k] = [stats.t.ppf(q=x,loc=mu,scale=s,df=5) for x in [0.5,0.975,0.025]]\n",
    "    #elif lfn_name == \"norm_nll\":\n",
    "    #    V = [stats.norm.ppf(q=x,loc=mu,scale=s) for x in qtiles]\n",
    "    #    us_ftest[k],us_utest[k],us_ltest[k] = [stats.norm.ppf(q=x,loc=mu,scale=s) for x in [0.5,0.975,0.025]]\n",
    "    #elif lfn_name == \"gamma_nll\":\n",
    "    ##\n",
    "    ## try using gamma err dist, even if trained on a different error fn?\n",
    "    ##\n",
    "    V = [stats.gamma.ppf(q=x, a=(mu*mu/s2) , scale=(s2/mu)) for x in qtiles]\n",
    "    us_ftest[k],us_utest[k],us_ltest[k] = [stats.gamma.ppf(q=x, a=(mu*mu/s2) , scale=(s2/mu)) for x in [0.5,0.975,0.025]]\n",
    "    us_qtest[k] = np.stack(V,axis=2) ## [series, time, quantiles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = \"median\"  #k = \"gamma_nll_0_0_908\"\n",
    "#plotpred(us_forecast, k, 0, us_train, us_test, horizon, us_lower, us_upper, cut-400)\n",
    "#plt.savefig(\"storage/\"+lfn_name+\"_US_\"+str(cut)+\".png\")\n",
    "plotpred(us_ftest, k, 0, us_train, us_test, horizon, us_ltest, us_utest, cut-400)\n",
    "#plt.savefig(\"storage/gamma_US_\"+str(cut)+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## process data for forecast hub\n",
    "\n",
    "## forecast date: output file will contain forecast for this day forward; default = current local date\n",
    "##\n",
    "## NOTE: model generates a forecast starting with the day after the training data ends,\n",
    "##   which may be in the past. But only forecast_date onward is written to the output file.\n",
    "train_end_date = pd.to_datetime(data_index[-1])\n",
    "\n",
    "if forecast_delay is not None:\n",
    "    forecast_date = pd.to_datetime(train_end_date + pd.Timedelta(days=forecast_delay))\n",
    "else:\n",
    "    forecast_date = pd.to_datetime(datetime.date.today()) \n",
    "\n",
    "\n",
    "## use ensembled forecasts; append US forecast derived above\n",
    "ensemble_tests = {\"t_nll\": np.concatenate([quantile_fc[\"median\"], us_quantile[\"median\"]],axis=0), ## [location, time, quantile]\n",
    "                  \"gamma\": np.concatenate([qtest[\"median\"], us_qtest[\"median\"]],axis=0)}\n",
    "\n",
    "\n",
    "for k in ensemble_tests:\n",
    "    q_ensemble = ensemble_tests[k]\n",
    "    location_codes = df_targ.columns.to_list() + [\"US\"] ## fips codes\n",
    "    quantile_labels = [f'{x:.3f}' for x in qtiles]\n",
    "    date_indices = pd.date_range(train_end_date + pd.Timedelta(days=1), train_end_date + pd.Timedelta(days=q_ensemble.shape[1]))\n",
    "\n",
    "    dfs = []\n",
    "    ## loop through each location in q_ensemble and make a dataframe with shape [date, value at each quantile]\n",
    "    for i in range(q_ensemble.shape[0]):\n",
    "        df = pd.DataFrame(q_ensemble[i,:,:])\n",
    "        df.columns = quantile_labels\n",
    "        df.index = date_indices\n",
    "        dfs.append(df.loc[forecast_date:,:].melt(ignore_index=False,var_name=\"quantile\").reset_index(names=\"target_end_date\"))\n",
    "\n",
    "    ## concatenate the location dataframes and set index to location code\n",
    "    df_hub = pd.concat(dfs,keys=location_codes).droplevel(1).reset_index(names=\"location\")\n",
    "\n",
    "    ## add the rest of the columns required by forecast hub\n",
    "    df_hub.loc[:,\"type\"] = \"quantile\"\n",
    "    df_hub.loc[:,\"forecast_date\"] = forecast_date\n",
    "    df_hub.loc[:,\"target\"] = df_hub.target_end_date.map(lambda d: str((d - forecast_date).days) + \" day ahead inc hosp\")\n",
    "    df_hub.loc[:,\"value\"] = df_hub.loc[:,\"value\"].round(2)\n",
    "\n",
    "    ##\n",
    "    ## TODO: try an error dist that doesn't allow negative values\n",
    "    ## for now, set them to 0\n",
    "    ##\n",
    "    df_hub.loc[df_hub[\"value\"]<0.0,\"value\"] = 0.0\n",
    "\n",
    "    # write to csv\n",
    "    hub_name = \"OHT_JHU-nbxd\"\n",
    "    filename = \"storage/\"+ k+\"_\" + forecast_date.strftime(\"%Y-%m-%d\") + \"-\" + hub_name + \".csv\"\n",
    "    print(\"writing \",filename)\n",
    "    df_hub.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(1000.0,200.0,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999.6284109626024, 40143.85509793832)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(x), np.var(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x12 = np.sqrt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31.451425338619384, 10.436255131852855)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(x12), np.var(x12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41729.508535417495"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(x12) * 4 * np.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://en.wikipedia.org/wiki/Taylor_expansions_for_the_moments_of_functions_of_random_variables\n",
    "## var(f(x)) ~~ (f'(mu))^2 * var(x) - 1/4(f''(mu))^2 * var(x)^2\n",
    "## delta method: var(f(x)) ~~ (f'(mu))^2 * var(x)\n",
    "\n",
    "## var x^2 ~~ (2mu)^2*var (delta method)\n",
    "## var x^2 ~~ (2mu)^2*var - var^2  (taylor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41184.93142953186"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = np.mean(x12)\n",
    "v = np.var(x12)\n",
    "\n",
    "4 * mu * mu * v - v*v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
